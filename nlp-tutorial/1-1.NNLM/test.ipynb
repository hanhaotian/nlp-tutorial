{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM2zrlkkXciB6L27ce1OKOK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanhaotian/nlp-tutorial/blob/master/nlp-tutorial/1-1.NNLM/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bsn4Br5Fm-kQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "33562c03-6456-400b-a4d7-37a02dc14ed5"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "dtype = torch.FloatTensor # FloatTensor类型\n",
        "\n",
        "sentences = ['I like milk', 'He love apple', 'She hate banana']\n",
        "\n",
        "# 输入层的词表|V|，即语料库中出现过的所有唯一词汇数量\n",
        "vocab_list = set(sorted([i for x in sentences for i in x.split(' ')]))  \n",
        "word_2_index = {w: i for i, w in enumerate(vocab_list)}\n",
        "index_2_word = {i: w for i, w in enumerate(vocab_list)}\n",
        "\n",
        "V = len(vocab_list) # 词表的大小\n",
        "n = len(sentences[0].split(' ')) - 1  # 窗口大小\n",
        "h = 3 # 隐藏层大小\n",
        "m = 3 # 特征大小\n",
        "\n",
        "class NNLM(nn.Module):\n",
        "  def __init__(self, V, n, h, m):\n",
        "    super().__init__()\n",
        "    # torch.nn.Embedding(num_embeddings, embedding_dim)\n",
        "    # a. 保存了固定字典和大小的简单查找表，通常用于保存词嵌入和用下标检索它们\n",
        "    # b. 用于创建一个词嵌入模型，num_embeddings代表一共有多少个词，embedding_dim表示为每个词创建多少维的向量\n",
        "    self.C = nn.Embedding(V, m)\n",
        "    print('C: ', self.C)\n",
        "    self.H = nn.Parameter(torch.zeros(m * n, h))\n",
        "    print('H: ', self.H)\n",
        "    self.d = nn.Parameter(torch.zeros(h))\n",
        "    print('d: ', self.d.size())\n",
        "    self.U = nn.Parameter(torch.zeros(h, V))\n",
        "    self.W = nn.Parameter(torch.zeros(n*m, V))\n",
        "    self.b = nn.Parameter(torch.zeros(V))\n",
        "\n",
        "  def forward(self, input):\n",
        "    x = self.C(input) # 3 x 2 x 3 每个词转换为指定维度\n",
        "    x = x.view(-1, n * m) # 连接操作 => 3 x 6\n",
        "    hidden = torch.tanh(torch.mm(x, self.H) + self.d) # torch.mm表示矩阵相乘 => 3 x 6 * 6 x 3 + 3 x 1\n",
        "    y = self.b + torch.mm(x, self.W) + torch.mm(hidden, self.U)\n",
        "    output = torch.softmax(y, 1)\n",
        "    return output\n",
        "\n",
        "input_batch = []\n",
        "output_batch = []\n",
        "\n",
        "for s in sentences:\n",
        "  words = s.split(' ')\n",
        "  input_batch.append([word_2_index[i] for i in words[:-1]])\n",
        "  output_batch.append(word_2_index[words[-1]])\n",
        "\n",
        "print(word_2_index)\n",
        "\n",
        "\n",
        "input_batch = torch.LongTensor(input_batch) # 3 x 2\n",
        "output_batch = torch.LongTensor(output_batch) # 3 x 1\n",
        "\n",
        "print(input_batch.size())\n",
        "print(output_batch.size())\n",
        "\n",
        "model = NNLM(V, n, h, m)\n",
        "\n",
        "# 神经网络优化器，主要是为了优化我们的神经网络，使它在我们训练过程中快起来，节省训练网络的时间\n",
        "# 优化器是需要根据网络反向传播的梯度信息来更新网络的参数，以起到降低loss函数计算值的作用。\n",
        "# a. 优化器需要知道当前网络或其他模型的参数空间，这也是为什么训练文件中，正式开始训练前，需要将网络的参数放到优化器里面\n",
        "# b. 需要知道反向传播的梯度信息\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # 学习率为0.01\n",
        "# 交叉熵损失函数\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "loss_list = []\n",
        "for epoch in range(1001):\n",
        "  output = model(input_batch)\n",
        "  # 计算损失函数\n",
        "  loss = criterion(output, output_batch)\n",
        "  # 将模型的参数梯度初始化为0，根据pythorch的backward函数计算，当网络参量进行反馈时，梯度是累积计算而不是被替换，但在处理每一个batch\n",
        "  # 时并不需要与其他batch的梯度混合起来累积计算。因此需要对每一个batch调用一遍zero_grad()将参数梯度置为0\n",
        "  optimizer.zero_grad() # 梯度设置为0\n",
        "  loss.backward() # 反向传播\n",
        "  optimizer.step()  # 用来更新优化器的学习率，一般按照epoch为单位进行更换，即多少个epoch后更新一次学习率，\n",
        "  loss_list.append(loss.data.numpy())\n",
        "\n",
        "  if epoch % 200 == 0:\n",
        "    print('Epoch {}: {}'.format(epoch, loss))\n",
        "    print('model.C: ', model.C)\n",
        "    print('model.H: ', model.H)\n",
        "    print('model.d: ', model.d)\n",
        "    print('model.U: ', model.U)\n",
        "    print('model.W: ', model.W)\n",
        "    print('\\n\\n')\n",
        "\n",
        "\n",
        "for index, input in enumerate(input_batch):\n",
        "  predict = model(input)\n",
        "  input_word = [index_2_word[int(index)] for index in input]\n",
        "  print('-'*40)\n",
        "  print('Predict: ', ' '.join(input_word), '->', index_2_word[int(torch.argmax(predict, dim=1))], '\\nTruth', sentences[index])\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'love': 0, 'apple': 1, 'like': 2, 'I': 3, 'milk': 4, 'He': 5, 'hate': 6, 'banana': 7, 'She': 8}\n",
            "torch.Size([3, 2])\n",
            "torch.Size([3])\n",
            "C:  Embedding(9, 3)\n",
            "H:  Parameter containing:\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]], requires_grad=True)\n",
            "d:  torch.Size([3])\n",
            "Epoch 0: 2.1972246170043945\n",
            "model.C:  Embedding(9, 3)\n",
            "model.H:  Parameter containing:\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]], requires_grad=True)\n",
            "model.d:  Parameter containing:\n",
            "tensor([0., 0., 0.], requires_grad=True)\n",
            "model.U:  Parameter containing:\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0.]], requires_grad=True)\n",
            "model.W:  Parameter containing:\n",
            "tensor([[ 1.4170e-04, -9.2302e-04,  1.4170e-04,  1.4170e-04,  6.0657e-05,\n",
            "          1.4170e-04,  1.4170e-04,  1.2181e-05,  1.4170e-04],\n",
            "        [ 4.8022e-05, -2.0877e-05,  4.8022e-05,  4.8022e-05, -5.0745e-04,\n",
            "          4.8022e-05,  4.8022e-05,  2.4019e-04,  4.8022e-05],\n",
            "        [ 9.2558e-05, -4.2999e-04,  9.2558e-05,  9.2558e-05, -7.9132e-05,\n",
            "          9.2558e-05,  9.2558e-05, -4.6226e-05,  9.2558e-05],\n",
            "        [ 1.8342e-05, -1.4779e-04,  1.8342e-05,  1.8342e-05,  2.0619e-04,\n",
            "          1.8342e-05,  1.8342e-05, -1.6845e-04,  1.8342e-05],\n",
            "        [ 4.4195e-05, -4.2588e-04,  4.4195e-05,  4.4195e-05, -1.1130e-04,\n",
            "          4.4195e-05,  4.4195e-05,  2.7201e-04,  4.4195e-05],\n",
            "        [-4.9281e-05,  2.3238e-04, -4.9281e-05, -4.9281e-05, -4.8976e-05,\n",
            "         -4.9281e-05, -4.9281e-05,  1.1228e-04, -4.9281e-05]],\n",
            "       requires_grad=True)\n",
            "\n",
            "\n",
            "\n",
            "Epoch 200: 2.0988800525665283\n",
            "model.C:  Embedding(9, 3)\n",
            "model.H:  Parameter containing:\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]], requires_grad=True)\n",
            "model.d:  Parameter containing:\n",
            "tensor([0., 0., 0.], requires_grad=True)\n",
            "model.U:  Parameter containing:\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0.]], requires_grad=True)\n",
            "model.W:  Parameter containing:\n",
            "tensor([[ 0.0411, -0.2916,  0.0411,  0.0411,  0.0280,  0.0411,  0.0411,  0.0172,\n",
            "          0.0411],\n",
            "        [ 0.0113, -0.0084,  0.0113,  0.0113, -0.1120,  0.0113,  0.0113,  0.0524,\n",
            "          0.0113],\n",
            "        [ 0.0251, -0.1374,  0.0251,  0.0251, -0.0107,  0.0251,  0.0251, -0.0026,\n",
            "          0.0251],\n",
            "        [ 0.0054, -0.0468,  0.0054,  0.0054,  0.0480,  0.0054,  0.0054, -0.0337,\n",
            "          0.0054],\n",
            "        [ 0.0144, -0.1329,  0.0144,  0.0144, -0.0180,  0.0144,  0.0144,  0.0645,\n",
            "          0.0144],\n",
            "        [-0.0133,  0.0744, -0.0133, -0.0133, -0.0146, -0.0133, -0.0133,  0.0201,\n",
            "         -0.0133]], requires_grad=True)\n",
            "\n",
            "\n",
            "\n",
            "Epoch 400: 1.9400993585586548\n",
            "model.C:  Embedding(9, 3)\n",
            "model.H:  Parameter containing:\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]], requires_grad=True)\n",
            "model.d:  Parameter containing:\n",
            "tensor([0., 0., 0.], requires_grad=True)\n",
            "model.U:  Parameter containing:\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0.]], requires_grad=True)\n",
            "model.W:  Parameter containing:\n",
            "tensor([[ 9.1125e-02, -6.6817e-01,  9.1125e-02,  9.1125e-02,  7.3033e-02,\n",
            "          9.1125e-02,  9.1125e-02,  4.8391e-02,  9.1125e-02],\n",
            "        [ 2.4754e-02, -1.2783e-02,  2.4754e-02,  2.4754e-02, -2.4730e-01,\n",
            "          2.4754e-02,  2.4754e-02,  1.1156e-01,  2.4754e-02],\n",
            "        [ 5.5123e-02, -3.1243e-01,  5.5123e-02,  5.5123e-02, -1.8739e-02,\n",
            "          5.5123e-02,  5.5123e-02,  4.3682e-04,  5.5123e-02],\n",
            "        [ 1.1805e-02, -1.0914e-01,  1.1805e-02,  1.1805e-02,  1.0766e-01,\n",
            "          1.1805e-02,  1.1805e-02, -6.9353e-02,  1.1805e-02],\n",
            "        [ 3.2827e-02, -3.0441e-01,  3.2827e-02,  3.2827e-02, -3.4026e-02,\n",
            "          3.2827e-02,  3.2827e-02,  1.4148e-01,  3.2827e-02],\n",
            "        [-2.9084e-02,  1.6998e-01, -2.9084e-02, -2.9084e-02, -3.4666e-02,\n",
            "         -2.9084e-02, -2.9084e-02,  3.9190e-02, -2.9084e-02]],\n",
            "       requires_grad=True)\n",
            "\n",
            "\n",
            "\n",
            "Epoch 600: 1.8739861249923706\n",
            "model.C:  Embedding(9, 3)\n",
            "model.H:  Parameter containing:\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]], requires_grad=True)\n",
            "model.d:  Parameter containing:\n",
            "tensor([0., 0., 0.], requires_grad=True)\n",
            "model.U:  Parameter containing:\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0.]], requires_grad=True)\n",
            "model.W:  Parameter containing:\n",
            "tensor([[ 0.1166, -0.8373,  0.1166,  0.1166,  0.0881,  0.1166,  0.1166,  0.0498,\n",
            "          0.1166],\n",
            "        [ 0.0395,  0.0086,  0.0395,  0.0395, -0.4238,  0.0395,  0.0395,  0.1782,\n",
            "          0.0395],\n",
            "        [ 0.0741, -0.3810,  0.0741,  0.0741, -0.0506,  0.0741,  0.0741, -0.0127,\n",
            "          0.0741],\n",
            "        [ 0.0135, -0.1422,  0.0135,  0.0135,  0.1772,  0.0135,  0.0135, -0.1159,\n",
            "          0.0135],\n",
            "        [ 0.0404, -0.3852,  0.0404,  0.0404, -0.0724,  0.0404,  0.0404,  0.2149,\n",
            "          0.0404],\n",
            "        [-0.0385,  0.2095, -0.0385, -0.0385, -0.0486, -0.0385, -0.0385,  0.0702,\n",
            "         -0.0385]], requires_grad=True)\n",
            "\n",
            "\n",
            "\n",
            "Epoch 800: 1.7977041006088257\n",
            "model.C:  Embedding(9, 3)\n",
            "model.H:  Parameter containing:\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]], requires_grad=True)\n",
            "model.d:  Parameter containing:\n",
            "tensor([0., 0., 0.], requires_grad=True)\n",
            "model.U:  Parameter containing:\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0.]], requires_grad=True)\n",
            "model.W:  Parameter containing:\n",
            "tensor([[ 0.1334, -0.9308,  0.1334,  0.1334,  0.0927,  0.1334,  0.1334,  0.0378,\n",
            "          0.1334],\n",
            "        [ 0.0588,  0.0470,  0.0588,  0.0588, -0.6590,  0.0588,  0.0588,  0.2590,\n",
            "          0.0588],\n",
            "        [ 0.0906, -0.4079,  0.0906,  0.0906, -0.1010,  0.0906,  0.0906, -0.0348,\n",
            "          0.0906],\n",
            "        [ 0.0125, -0.1667,  0.0125,  0.0125,  0.2666,  0.0125,  0.0125, -0.1747,\n",
            "          0.0125],\n",
            "        [ 0.0441, -0.4331,  0.0441,  0.0441, -0.1291,  0.0441,  0.0441,  0.2974,\n",
            "          0.0441],\n",
            "        [-0.0461,  0.2278, -0.0461, -0.0461, -0.0626, -0.0461, -0.0461,  0.1113,\n",
            "         -0.0461]], requires_grad=True)\n",
            "\n",
            "\n",
            "\n",
            "Epoch 1000: 1.6992965936660767\n",
            "model.C:  Embedding(9, 3)\n",
            "model.H:  Parameter containing:\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]], requires_grad=True)\n",
            "model.d:  Parameter containing:\n",
            "tensor([0., 0., 0.], requires_grad=True)\n",
            "model.U:  Parameter containing:\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0.]], requires_grad=True)\n",
            "model.W:  Parameter containing:\n",
            "tensor([[ 0.1467, -0.9981,  0.1467,  0.1467,  0.1003,  0.1467,  0.1467,  0.0176,\n",
            "          0.1467],\n",
            "        [ 0.0798,  0.0885,  0.0798,  0.0798, -0.9235,  0.0798,  0.0798,  0.3560,\n",
            "          0.0798],\n",
            "        [ 0.1069, -0.4195,  0.1069,  0.1069, -0.1563,  0.1069,  0.1069, -0.0658,\n",
            "          0.1069],\n",
            "        [ 0.0110, -0.1860,  0.0110,  0.0110,  0.3678,  0.0110,  0.0110, -0.2475,\n",
            "          0.0110],\n",
            "        [ 0.0447, -0.4732,  0.0447,  0.0447, -0.1923,  0.0447,  0.0447,  0.3972,\n",
            "          0.0447],\n",
            "        [-0.0536,  0.2373, -0.0536, -0.0536, -0.0792, -0.0536, -0.0536,  0.1637,\n",
            "         -0.0536]], requires_grad=True)\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------\n",
            "Predict:  I like -> milk \n",
            "Truth I like milk\n",
            "----------------------------------------\n",
            "Predict:  He love -> apple \n",
            "Truth He love apple\n",
            "----------------------------------------\n",
            "Predict:  She hate -> banana \n",
            "Truth She hate banana\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZNl-9u5nIyp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 类型转换，将list、numpy转化为tensor\n",
        "print(torch.FloatTensor([1, 2]))\n",
        "# 根据torch.size()创建一个tensor\n",
        "a = torch.tensor([[1, 2], [3, 4]])\n",
        "print(torch.FloatTensor(a.size()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2gI7s9SnLx2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "nn.Embedding测试1\n",
        "'''\n",
        "from torch.autograd import Variable\n",
        "word_2_ix = {'hello': 0, 'word': 1}\n",
        "embeds = nn.Embedding(2, 5) # 这里的词向量的建立知识初始的词向量，并没有经过任何修改优化\n",
        "print(embeds)\n",
        "hello_idx = Variable(torch.LongTensor([word_2_ix['hello']])) # 得到一个Variable，值为hello的索引0\n",
        "print(hello_idx)\n",
        "print(embeds(hello_idx))  # 得到word embedding中关于hello这个词的初始词向量\n",
        "word_idx = Variable(torch.LongTensor([word_2_ix['word']]))\n",
        "print(word_idx)\n",
        "print(embeds(word_idx))\n",
        "\n",
        "'''\n",
        "nn.Embedding测试2\n",
        "'''\n",
        "embeds = nn.Embedding(5, 4) # 假定字典中只有5个词，词向量维度为4\n",
        "# 每个数字代表一个词，如{'!': 0, 'how': 1, 'are': 2, 'you': 3, 'ok':4}，且这些数字的范围只能在0-4之间，因为上面定义了只有5个词\n",
        "word = [[1, 2, 3], [2, 3, 4]]\n",
        "embed = embeds(torch.LongTensor(word)) \n",
        "print(embed)\n",
        "print(embed.size()) # torch.Size([2, 3, 4])表示对于输入的[2, 3]维的词，每一个词都被映射成了一个4维的向量\n",
        "\n",
        "'''\n",
        "torch.zeros测试\n",
        "'''\n",
        "print(torch.zeros(3))\n",
        "\n",
        "'''\n",
        "tensor.view变换\n",
        "'''\n",
        "# view返回一个有相同数据但大小不同的tensor，即进行reshape操作\n",
        "x = torch.randn(3, 4, 5, 7)\n",
        "print('tensor原型:' , x)\n",
        "# -1表示剩下的值的个数一起构成一个维度，例如，第一个参数1将第一个维度设定为1，后一个-1即第二个维度的大小=元素总数目/第一个维度的大小，此例为3*4*5*7/1=420\n",
        "b = x.view(1, -1)\n",
        "print(b.size())\n",
        "d = x.view(x.size(0), x.size(1), -1) # 3*4*5*7/(3*4)=35\n",
        "print(d.size())\n",
        "e = x.view(4, -1, 5) # 3*4*5*7/(4*5) = 21\n",
        "print(e.size())\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}